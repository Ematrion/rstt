{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a784930b-9ef3-4305-99c6-946b53669eae",
   "metadata": {},
   "source": [
    "# RSTT Tutorial 2\n",
    "## Integration\n",
    "\n",
    "In this notebook we will use the [openskill](https://openskill.me/en/stable/) rating system with RSTT.\n",
    "The goal is to wrapp model in a Ranking class to benefit from its functionnalities and fit in simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae5d91d-45c5-4833-bc44-8965adc76db1",
   "metadata": {},
   "source": [
    "## 1. RSTT Ranking Design \n",
    "\n",
    "A [Ranking](https://rstt.readthedocs.io/en/latest/rstt.ranking.html#rstt.ranking.ranking.Ranking) is a composition over inheritance design that contains:\n",
    "- A **[Standing](https://rstt.readthedocs.io/en/latest/rstt.ranking.html#rstt.ranking.standing.Standing)**: dict/list container hybrid. **Automaticaly sorts player** based on their *ranking point*\n",
    "- A **[RatingSystem](https://rstt.readthedocs.io/en/latest/rstt.html#rstt.stypes.RatingSystem)**: dict like container that **maps player with ratings**\n",
    "- An **[Inference](https://rstt.readthedocs.io/en/latest/rstt.html#rstt.stypes.Inference)**: provide a **.rate method()** to compute ratings\n",
    "- An **[Observer](https://rstt.readthedocs.io/en/latest/rstt.html#rstt.stypes.Observer)**: provide an **.handle_observations()** method that process ranking.update input\n",
    "\n",
    "Before integrating external system, lets start with a simple illustration.\n",
    "A ranking can be instanciated with its components specified. However, we recommand to represent a ranking design in its own class. It makes it more clear what parameters are intresect to the ranking design, and which are hyper-parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c526ce-14ae-4acb-876d-91dbd73bf42f",
   "metadata": {},
   "source": [
    "#### 1.1 Simple instanciation\n",
    "\n",
    "A ranking can be instanciated with its components specified. NOT RECOMMANDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d6e7d61-2333-4713-baae-b7fb12244573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rstt.ranking.ranking.Ranking at 0x1063ff350>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rstt import Ranking\n",
    "from rstt.ranking import KeyModel, Elo, GameByGame\n",
    "\n",
    "# Ambiguity about between core element and parameters. Is the handler a tunable parameter of the ranking?\n",
    "Ranking(name='elo', datamodel=KeyModel(default=1000), backend=Elo(k=20), handler=GameByGame())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8354b511-8b40-4690-b54f-f0ca04218c7d",
   "metadata": {},
   "source": [
    "#### 1.2 Class Design\n",
    "\n",
    "We recommand to represent a ranking design in its own class with an explicit naming. It makes it more clear what parameters are inherent to the ranking design, and which are tunable hyper-parameters for comparative studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96cac3a4-ace7-43b2-9f46-2d7d687ebab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distinguish core design from parameters, handler is not a parameter.\n",
    "class EloGBG(Ranking):\n",
    "    def __init__(self, name: str, default_rating: float=1000, k: float=20):\n",
    "        # The standing component provided in the super() init.\n",
    "        super().__init__(name=name,\n",
    "                         datamodel=KeyModel(default=default_rating), # RatingSystem\n",
    "                         backend=Elo(k=20), # Inference\n",
    "                         handler=GameByGame()) # Observer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31e9917-6cf3-40dc-bc56-84afcee881e3",
   "metadata": {},
   "source": [
    "#### 1.3 Run illustration\n",
    "\n",
    "As you can see, there is not much to do and it works just fine in simulation. The RSTT built-in [BasicElo]() class code is in fact very similar.\n",
    "All ranking's functionalities are implemented at a higher level of abstraction and relies on minimal requirements from its components to work as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96fba075-aab5-4ff9-8eee-9cc632e05425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- elo -----------\n",
      "   0.       Veronica Clark       1219\n",
      "   1.     Stephanie Jordan       1191\n",
      "   2.           Chad Brown       1186\n",
      "   3.     Christopher Cade       1172\n",
      "   4.          Mable Baker       1143\n",
      "   5.          Donna Mckee       1143\n",
      "   6.        Margaret Bass       1132\n",
      "   7.       Ashley Sanchez       1099\n",
      "   8.         Ryan Pickard       1085\n",
      "   9.        Steven Dupree       1085\n",
      "  10.      Kathryn Coleman       1056\n",
      "  11.       Timothy Vargas       1051\n",
      "  12.          Hilda Smith       1036\n",
      "  13.        Angela Mccray       1015\n",
      "  14.      Virginia Seeley       1005\n",
      "  15.        Marina Thomas        990\n",
      "  16.       Randy Richards        989\n",
      "  17.          David Bowen        972\n",
      "  18.        Scott Dobbins        963\n",
      "  19.        Alyson Curiel        962\n",
      "  20.        Joyce Calisto        941\n",
      "  21.         Ana Valencia        926\n",
      "  22.      Allyson Johnson        922\n",
      "  23.        Todd Crawford        901\n",
      "  24.     Seymour Frerichs        899\n",
      "  25.     Charles Mitchell        884\n",
      "  26.          John Lawson        871\n",
      "  27.        Gregory Saari        867\n",
      "  28.         Pearl Crouse        866\n",
      "  29.          Kevin Dryer        820\n",
      "  30.          Glady Davis        800\n",
      "  31.         Peter Chavez        791\n"
     ]
    }
   ],
   "source": [
    "from rstt import Player, RoundRobin, LogSolver\n",
    "\n",
    "# our ranking design\n",
    "elo = EloGBG('elo')\n",
    "\n",
    "# players\n",
    "population = Player.create(nb=32)\n",
    "\n",
    "# games\n",
    "tournament = RoundRobin('test', elo, LogSolver())\n",
    "tournament.registration(population)\n",
    "tournament.run()\n",
    "\n",
    "# check if it works\n",
    "elo.update(games=tournament.games())\n",
    "elo.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a6c9c2-485e-40ea-b603-d6c7fb2aad36",
   "metadata": {},
   "source": [
    "## 2. Use OpenSkill in RSTT\n",
    "\n",
    "[Openskill](https://github.com/vivekjoshy/openskill.py) is an Inference system according to RSTT terminology.\n",
    "On Github, it encourages to drop TrueSkill and Elo.\n",
    "So ... lets test it!\n",
    "\n",
    "#### 2.1 Ranking.datamodel: stypes.RatingSystem\n",
    "It acts as a container of rating object. It must provide get and set method for player's rating. It also provides a float interpretation of rating with an ordinal funciton. Lets first take a look at openskill rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0420fabf-ec35-4aeb-9e7c-ade1b0f07acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: None id: 8dddf85814d54895b9bce6fa52de4d2b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PlackettLuceRating(mu=25.0, sigma=8.333333333333334)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openskill.models import PlackettLuce, BradleyTerryFull, BradleyTerryPart, ThurstoneMostellerFull, ThurstoneMostellerPart\n",
    "\n",
    "model = PlackettLuce()\n",
    "rating = model.rating()\n",
    "print('name:', rating.name, 'id:', rating.id)\n",
    "rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3915db97-3a14-44a3-a3f4-d990e06d30e6",
   "metadata": {},
   "source": [
    "#### 2.2 KeyModel, a general purpose RatingSystem\n",
    "\n",
    "The KeyModel class is a base class for the RatingSystem protocol (see elo example). It provides all features needed and just require you to provide a default rating (for player that do not have one yet). \n",
    "\n",
    "There are 3 way to specify a default rating\n",
    "- by providing a value: **default** = model.rating()\n",
    "- by providing a constructor: **template** = model.rating\n",
    "- by providing a function which takes as input the player for which a rating is created: **factory** = lambda player: model.rating(name=player.name()\n",
    "\n",
    "\n",
    "In the case of openskill, since rating do contain an id, it is better to avoid the default approach. The template is an option, but since rating have names, why not make it match the one player.name()? Let us use the factory approach.\n",
    "\n",
    "KeyModel has a basic ordinal implementation that will not work here. We need to overite it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e3d5094-8636-4da1-9ae3-35e82b55622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rstt.ranking import KeyModel\n",
    "\n",
    "class OSRatings(KeyModel):\n",
    "    def __init__(self, model):\n",
    "        super().__init__(factory= lambda x, **kwargs: model.rating(name=x.name(), **kwargs), mu=40, sigma =5)\n",
    "\n",
    "    def ordinal(self, rating) -> float:\n",
    "        # openskill ratings have an ordinal functionality themself - easy !\n",
    "        return rating.ordinal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6d10a68-300d-4f1f-bdee-71f1a9a83571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PlackettLuceRating(mu=25.0, sigma=8.333333333333334)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "osr = OSRatings(PlackettLuce())\n",
    "osr.get(Player('dummy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634cd601-3630-4753-b433-0ed84ffdd084",
   "metadata": {},
   "source": [
    "#### 2.2 Ranking.backend: stypes.Inference\n",
    "\n",
    "Inference is a notion define as a Protocol and typechecked in the RSTT package.\n",
    "Anything that provide a .rate() method fits the bill. Openskill.models have all a .rate method thus are RSTT.stypes.Inference and can directly be passed to a ranking class as backend. Nothing to do. Cool!\n",
    "\n",
    "This is not always the case. You can however write a simple class with a rate method that wrapps the rate process of the system to intergrate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dc462d-a52d-4f0b-9d6d-408d1e994011",
   "metadata": {},
   "source": [
    "#### 2.3 Ranking.handler: stypes.Observer\n",
    "\n",
    "The handler.handle_observations() method is called by the ranking.forward() during the ranking.update() execution. \n",
    "\n",
    "- Ranking.update is a user level functionnality that should **NEVER** be override.\n",
    "- Ranking.forward is a develloper functionnality. It **CAN** be override, usualy not necessary.\n",
    "- Observer.handle_observations deals is a complete workflow from the update input to the new ranking state\n",
    "\n",
    "In a majority of cases, the handle_observations perform the following steps:\n",
    "1) Format the update inputs. The inputs are referred as 'observations'. They justify a change of ranking state.\n",
    "2) Extract from the observations the relevant information\n",
    "3) Query the datamodel for the corresponding *prior* ratings\n",
    "4) Call the backend.rate method with correct arguments\n",
    "5) Interpret the backend.rate return values\n",
    "6) Push the *posteriori* ratings to the datamodel\n",
    "\n",
    "We want to input a list of RSTT.stypes.SMatch. We already have work on the ratings in the datamodel.\n",
    "We need to extract relevant data from games. So we need to know what to pass to the rate method. Lets have a look at its signature. Your Task is to read the Observer code and identify the 6 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93ab3b2c-f0a7-49e3-b8d0-9548a179b106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'return': typing.List[typing.List[openskill.models.weng_lin.plackett_luce.PlackettLuceRating]],\n",
       " 'teams': typing.List[typing.List[openskill.models.weng_lin.plackett_luce.PlackettLuceRating]],\n",
       " 'ranks': typing.Optional[typing.List[float]],\n",
       " 'scores': typing.Optional[typing.List[float]],\n",
       " 'tau': typing.Optional[float],\n",
       " 'limit_sigma': typing.Optional[bool]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "inspect.getfullargspec(model.rate).annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21f27d25-407a-479b-abc5-ebc2c6de423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rstt.stypes import RatingSystem, Inference, SMatch\n",
    "\n",
    "class OSHandler:\n",
    "    def handle_observations(self, datamodel: RatingSystem, infer: Inference, games: list[SMatch]):\n",
    "        for game in games:\n",
    "            # extract game info\n",
    "            teams_of_players = game.teams()\n",
    "            scores = game.scores() # alternative: ranks = game.ranks()\n",
    "            \n",
    "            # get corresponding rating from datamodel\n",
    "            teams = [] # list[list[rating]]\n",
    "            for team in teams_of_players:\n",
    "                ratings = [] # list[rating]\n",
    "                for player in team:\n",
    "                    ratings.append(datamodel.get(player))\n",
    "                teams.append(ratings)\n",
    "            \n",
    "            # call rate\n",
    "            new_ratings = infer.rate(teams=teams, scores=scores) # or ..., ranks=ranks)\n",
    "            \n",
    "            # push new ratings\n",
    "            for team, ratings in zip(teams_of_players, new_ratings):\n",
    "                for player, rating in zip(team, ratings):\n",
    "                    datamodel.set(player, rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1abfd0a-0693-4004-af1a-78150ece5cdc",
   "metadata": {},
   "source": [
    "#### 2.4 Run illustration\n",
    "\n",
    "The OpenSkill Ranking class will take one single parameter, an openskill.models object. And then it is ready to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aef242e6-354c-4630-8494-94bfa94fed5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- OpenSkill -----------\n",
      "   0.       Veronica Clark         37\n",
      "   1.     Stephanie Jordan         33\n",
      "   2.           Chad Brown         32\n",
      "   3.     Christopher Cade         31\n",
      "   4.          Donna Mckee         27\n",
      "   5.          Mable Baker         27\n",
      "   6.        Margaret Bass         25\n",
      "   7.       Ashley Sanchez         21\n",
      "   8.         Ryan Pickard         20\n",
      "   9.        Steven Dupree         19\n",
      "  10.      Kathryn Coleman         18\n",
      "  11.       Timothy Vargas         16\n",
      "  12.          Hilda Smith         14\n",
      "  13.        Angela Mccray         13\n",
      "  14.      Virginia Seeley         11\n",
      "  15.        Marina Thomas         10\n",
      "  16.       Randy Richards          9\n",
      "  17.        Alyson Curiel          8\n",
      "  18.          David Bowen          7\n",
      "  19.        Scott Dobbins          7\n",
      "  20.        Joyce Calisto          4\n",
      "  21.         Ana Valencia          3\n",
      "  22.      Allyson Johnson          2\n",
      "  23.     Seymour Frerichs          0\n",
      "  24.        Todd Crawford          0\n",
      "  25.     Charles Mitchell         -1\n",
      "  26.          John Lawson         -3\n",
      "  27.        Gregory Saari         -3\n",
      "  28.         Pearl Crouse         -4\n",
      "  29.          Kevin Dryer        -10\n",
      "  30.          Glady Davis        -13\n",
      "  31.         Peter Chavez        -16\n"
     ]
    }
   ],
   "source": [
    "class OpenSKill(Ranking):\n",
    "    def __init__(self, name: str, model):\n",
    "        super().__init__(name=name, datamodel=OSRatings(model), backend=model, handler=OSHandler())\n",
    "\n",
    "os = OpenSKill('OpenSkill', model)\n",
    "os.update(games=tournament.games())\n",
    "os.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794ff975-63f1-4fa4-ad52-378bfd67ac1a",
   "metadata": {},
   "source": [
    "## 3. Ranking functionality\n",
    "\n",
    "This is now openskill on steroïds. You can access playesr by ranks, get rating of a player You can use it to seed competition like a single elimination bracket. Lets start by a simple standard output plot of the standing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8832f74-66e0-42f5-8234-d9a28cf14b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- OpenSkill -----------\n",
      "   0.       Veronica Clark         37\n",
      "   1.     Stephanie Jordan         33\n",
      "   2.           Chad Brown         32\n",
      "   3.     Christopher Cade         31\n",
      "   4.          Donna Mckee         27\n",
      "   5.          Mable Baker         27\n",
      "   6.        Margaret Bass         25\n",
      "   7.       Ashley Sanchez         21\n",
      "   8.         Ryan Pickard         20\n",
      "   9.        Steven Dupree         19\n",
      "  10.      Kathryn Coleman         18\n",
      "  11.       Timothy Vargas         16\n",
      "  12.          Hilda Smith         14\n",
      "  13.        Angela Mccray         13\n",
      "  14.      Virginia Seeley         11\n",
      "  15.        Marina Thomas         10\n",
      "  16.       Randy Richards          9\n",
      "  17.        Alyson Curiel          8\n",
      "  18.          David Bowen          7\n",
      "  19.        Scott Dobbins          7\n",
      "  20.        Joyce Calisto          4\n",
      "  21.         Ana Valencia          3\n",
      "  22.      Allyson Johnson          2\n",
      "  23.     Seymour Frerichs          0\n",
      "  24.        Todd Crawford          0\n",
      "  25.     Charles Mitchell         -1\n",
      "  26.          John Lawson         -3\n",
      "  27.        Gregory Saari         -3\n",
      "  28.         Pearl Crouse         -4\n",
      "  29.          Kevin Dryer        -10\n",
      "  30.          Glady Davis        -13\n",
      "  31.         Peter Chavez        -16\n"
     ]
    }
   ],
   "source": [
    "os.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83069a82-db05-4207-9e9b-76ab0b7e1199",
   "metadata": {},
   "source": [
    "#### 3.1 Rank Correlation\n",
    "\n",
    "RSTT ranking interface simplifies some metrics compuation, like rank correlation. The advantage of simulation is that you have a baseline to comupte it. Lets compare elo, openskill and the simulation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acbf573c-e104-4fdf-9a76-04b864cbe811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenSkill - GroundTRuth correlation: \n",
      "   SignificanceResult(statistic=np.float64(0.9233870967741936), pvalue=np.float64(1.8369284310000514e-22))\n",
      "Elo - GroundTRuth correlation: \n",
      "   SignificanceResult(statistic=np.float64(0.931451612903226), pvalue=np.float64(2.6719535498432205e-23))\n",
      "OpenSkill - Elo correlation: \n",
      "   SignificanceResult(statistic=np.float64(0.9838709677419356), pvalue=np.float64(3.9371288142144177e-31))\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "from rstt import BTRanking\n",
    "\n",
    "# ranking where players ratings are their respectives level(). \n",
    "gt = BTRanking('consensus', population)\n",
    "\n",
    "print('OpenSkill - GroundTRuth correlation: \\n  ', stats.kendalltau(gt[population], os[population]))\n",
    "print('Elo - GroundTRuth correlation: \\n  ', stats.kendalltau(gt[population], elo[population]))\n",
    "print('OpenSkill - Elo correlation: \\n  ', stats.kendalltau(elo[population], os[population]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d880a68-de8c-40a7-864d-093987f7e120",
   "metadata": {},
   "source": [
    "#### 3.2 Ranking state as simulation parameter\n",
    "\n",
    "You can easly play arround with the inital state of any RSTT ranking by provding an arbitrary ordering of the players involved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b258379a-e748-4225-ae78-8a5733b7a18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
      "[2, 12, 1, 17, 18, 26, 16, 24, 25, 19, 31, 3, 20, 11, 27, 15, 29, 9, 4, 10, 7, 23, 30, 13, 21, 22, 8, 5, 0, 14, 28, 6]\n",
      "Seeds - Truth correlation: -0.036290322580645164\n",
      "OpenSkill - GroundTRuth correlation: 0.0\n",
      "Elo - GroundTRuth correlation: 0.0\n",
      "OpenSkill - Elo correlation: 0.8467741935483872\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "seeds = list(range(len(os)))\n",
    "random.shuffle(seeds)\n",
    "\n",
    "print(list(range(len(os))))\n",
    "print(seeds)\n",
    "print('Seeds - Truth correlation:', stats.kendalltau(seeds, list(range(len(os)))).statistic)\n",
    "\n",
    "elo.rerank(seeds)\n",
    "os.rerank(seeds)\n",
    "print('OpenSkill - GroundTRuth correlation:', stats.kendalltau(gt[population], os[population]).statistic)\n",
    "print('Elo - GroundTRuth correlation:', stats.kendalltau(gt[population], elo[population]).statistic)\n",
    "print('OpenSkill - Elo correlation:', stats.kendalltau(elo[population], os[population]).statistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ade7bac-1f56-4129-b3fb-2e3dfdbe7ede",
   "metadata": {},
   "source": [
    "#### 3.3 Control the Interplay between a Ranking and a Dataset\n",
    "\n",
    "Now it is possible to select players and seed them in a competition based on their openskill ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7a9306c-aedf-42df-8ca7-b6660fa54768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rstt import SwissRound\n",
    "\n",
    "t2 = SwissRound(name='OpensKill seeded tournament', seeding=os, solver=LogSolver())\n",
    "t2.registration(os[:16]) # top 16 players according to openskill\n",
    "t2.run()\n",
    "os.update(games=t2.games())\n",
    "elo.update(games=t2.games())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfa3bce-3b0f-47fe-87c4-2e60888edf43",
   "metadata": {},
   "source": [
    "#### 3.4 Fancy Analisys\n",
    "\n",
    "Let see what changed. Keep in mind that we atrificialy changed the entire ranking state, but only a fraction of the players where involved in the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4bad345-d9d6-49cf-906d-c863ac94d5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Kendalltau rank correaltion on the entire population --\n",
      "OpenSkill - GroundTRuth correlation: 0.060483870967741944\n",
      "Elo - GroundTRuth correlation: 0.040322580645161296\n",
      "OpenSkill - Elo correlation: 0.8427419354838711\n",
      "\n",
      " -- Kendalltau rank correaltion on the real top16 --\n",
      "OpenSkill - GroundTRuth correlation: 0.5499999999999999\n",
      "Elo - GroundTRuth correlation: 0.5333333333333333\n",
      "OpenSkill - Elo correlation: 0.8499999999999999\n",
      "\n",
      " -- Kendalltau rank correaltion on the 'openskill prio' top16 --\n",
      "OpenSkill - GroundTRuth correlation: 0.35\n",
      "Elo - GroundTRuth correlation: 0.26666666666666666\n",
      "OpenSkill - Elo correlation: 0.7833333333333333\n"
     ]
    }
   ],
   "source": [
    "print('-- Kendalltau rank correaltion on the entire population --')\n",
    "print('OpenSkill - GroundTRuth correlation:', stats.kendalltau(gt[population], os[population]).statistic)\n",
    "print('Elo - GroundTRuth correlation:', stats.kendalltau(gt[population], elo[population]).statistic)\n",
    "print('OpenSkill - Elo correlation:', stats.kendalltau(elo[population], os[population]).statistic)\n",
    "\n",
    "print('\\n -- Kendalltau rank correaltion on the real top16 --')\n",
    "top16 = gt[:16]\n",
    "print('OpenSkill - GroundTRuth correlation:', stats.kendalltau(gt[top16], os[top16]).statistic)\n",
    "print('Elo - GroundTRuth correlation:', stats.kendalltau(gt[top16], elo[top16]).statistic)\n",
    "print('OpenSkill - Elo correlation:', stats.kendalltau(elo[top16], os[top16]).statistic)\n",
    "\n",
    "print('\\n -- Kendalltau rank correaltion on the \\'openskill prio\\' top16 --')\n",
    "seed16 = t2.participants()\n",
    "print('OpenSkill - GroundTRuth correlation:', stats.kendalltau(gt[seed16], os[seed16]).statistic)\n",
    "print('Elo - GroundTRuth correlation:', stats.kendalltau(gt[seed16], elo[seed16]).statistic)\n",
    "print('OpenSkill - Elo correlation:', stats.kendalltau(elo[seed16], os[seed16]).statistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de8b4a8-d5fb-4dda-84bc-d73b3b5ccfcd",
   "metadata": {},
   "source": [
    "## 4. Your Turn - Trueskill\n",
    "\n",
    "[Trueskill](https://trueskill.org) is also an RSTT.stypes.Inference. You know how to use it now!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b15b19-a457-4fd1-8a2d-6be843d36751",
   "metadata": {},
   "source": [
    "## 5. Your Turn - Real Data\n",
    "\n",
    "If I tell you it is not hard to run rstt ranking on real dataset, do you have an idea how to make it work?\n",
    "\n",
    "That is right. You write an oberserver! The component that deals with the update input. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
