{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3385030d-4ac2-4dfa-ab2f-899782a7a4e7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# RSTT Tutorial 3\n",
    "## Convergence of the Elo Rating System\n",
    "\n",
    "This tutorial is based on the paper from Adrien Krifa, Florian Spinelli and Stéphane Junca [On the Convergence of the Elo rating system for a Bernoulli model](https://hal.science/hal-03286065/document)\n",
    "\n",
    "The goal is to reproduce the simulation results of their work using python and the rstt package. The notebook follows the paper structure and focus on making bridges between the text and the package features.\n",
    "\n",
    "This tutorials contains conding and reading exercises that assume you already have some experience with the rstt package (take a look at our previous tutorials). Code cells start with a comment label indicating its content.\n",
    "- \\# READ: code should be understood to continue coding.\n",
    "- \\# FEATURE: highligh an unusal rstt feature.\n",
    "- \\# TODO: will not run unless you fill the [...].\n",
    "- \\# PLOT: we use matplotlib to replicate figure styling. It may not be of your interest.\n",
    "- \\# EXAMPLE: serves as runable example for later \\# TODO cells.\n",
    "\n",
    "We use a simple data structure to store experimental results - a dictionary mapping experimental title to ranking status over time.\n",
    "The utils module contains init/track_results functions and all plot cells are coded. You do not have to deal with this part of the research. You can **focus on coding the simulation model**. \n",
    "\n",
    "You can use dataframes for example, but you will need to adapt the entire notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "daeb396e-c9b0-4483-ae19-c3fbc80558e5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 1. Introduction\n",
    "\n",
    "RSTT provides a [BasicElo](https://rstt.readthedocs.io/en/latest/rstt.ranking.standard.html#module-rstt.ranking.standard.basicElo) ranking, using the [Elo](https://rstt.readthedocs.io/en/latest/rstt.ranking.inferer.html#module-rstt.ranking.inferer.elo) rating system.\n",
    "You can verify that the implementation satisfies the model in the paper:\n",
    "\n",
    "- (Eq. 1) & (Eq. 2) are implemented by the [Elo.update_rating](https://rstt.readthedocs.io/en/latest/rstt.ranking.inferer.html#rstt.ranking.inferer.elo.Elo.update_rating) method.\n",
    "- (Eq. 3) K=20 is commonly used, thus the default value in the package.\n",
    "- (Eq. 9) is implemented by the [Elo.expectedScore](https://rstt.readthedocs.io/en/latest/rstt.ranking.inferer.html#rstt.ranking.inferer.elo.Elo.expectedScore) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad82171-5295-4227-a7f1-dd331ae49d93",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FEATURE - Elo expected score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from rstt import BasicElo\n",
    "\n",
    "# an elo ranking\n",
    "elo = BasicElo(name='Elo Tutorial', lc=400, k=20)\n",
    "\n",
    "# illustrate the b function\n",
    "x = list(range(-500, 500))\n",
    "y = [elo.backend.expectedScore(rating1=1000, rating2=1000-diff) for diff in x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96985246-4633-4072-a56e-6e2a7d504f80",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PLOT\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y, color='blue')\n",
    "ax.plot(x, [1.0 for i in x], '--', color='blue')\n",
    "ax.plot(x, [0.5 for i in x], ':', color='red')\n",
    "ax.plot(x, [0.0 for i in x], '--', color='blue')\n",
    "ax.axvline(x=7, color='black')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('b(x)')\n",
    "plt.title('function b')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d35f6f67-8bd7-4fa4-99c3-bea103149c31",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 2. Two Player\n",
    "\n",
    "In RSTT, any object providing a .name() and a .level() method are [SPlayer](), they can be register in Rankings, take part in Competition and play games. When the **strenght of player is constant** you can use either the [BasicPlayer]() or the [Player]() class. [Solver]() are an essential part of a simulation, responsible to assign a [Score]() to a [SMatch](). There are several different solvers available, but the one needed here is the [LogSolver](), which matches the Elo.expectedScore.\n",
    "\n",
    "- **results of the matches are independant**\n",
    "- **the expected result of an encounter depends only on the strenght difference**\n",
    "- it uses the function b(x) = 1/(1+10^(-x/400)) where x is the difference in player levels.\n",
    "\n",
    "Remark on the LogSolver: the base (by default 10) and the constant (by default 400) are tunable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7ceec6-7ac7-40e0-888a-ca89bc56bae5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EXAMPLE\n",
    "\n",
    "from rstt import BasicPlayer, Duel, LogSolver\n",
    "\n",
    "# A standard for results\n",
    "results = {}\n",
    "title2 = \"Elo Simulation, with p=b(rho1-rho2)\"\n",
    "\n",
    "# model\n",
    "playerA, playerB = BasicPlayer('pA', 1500), BasicPlayer('pB', 700)\n",
    "solver = LogSolver()\n",
    "\n",
    "# parameters\n",
    "depth = 1000\n",
    "\n",
    "# initialization\n",
    "elo.set_rating(key=playerA, rating=1000)\n",
    "elo.set_rating(key=playerB, rating=1200)\n",
    "results[title2] = {p: {'elo': [elo.rating(p)],\n",
    "                       'rho': [p.level()]}\n",
    "                   for p in [playerA, playerB]}\n",
    "\n",
    "# simulation\n",
    "for i in range(depth):\n",
    "    # game generation\n",
    "    duel = Duel(player1=playerA, player2=playerB) # LogSolver does not have an 'home/away' bias\n",
    "    solver.solve(duel)\n",
    "\n",
    "    # ranking update\n",
    "    elo.update(game=duel)\n",
    "\n",
    "    # results tracking\n",
    "    results[title2][playerA]['elo'].append(elo.rating(playerA))\n",
    "    results[title2][playerA]['rho'].append(playerA.level())\n",
    "    results[title2][playerB]['elo'].append(elo.rating(playerB))\n",
    "    results[title2][playerB]['rho'].append(playerB.level())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556460d6-92aa-4404-a145-b351141c59ea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PLOT\n",
    "fig, ax = plt.subplots(figsize=(15, 4))\n",
    "x = list(range(depth+1))\n",
    "\n",
    "ax.plot(x, results[title2][playerA]['elo'], color='red')\n",
    "ax.plot(x, results[title2][playerA]['rho'], ':', color='red')\n",
    "ax.plot(x, results[title2][playerB]['elo'], color='blue')\n",
    "ax.plot(x, results[title2][playerB]['rho'], ':', color='blue')\n",
    "\n",
    "ax.set_xlabel('i-th game')\n",
    "ax.set_xticks(list(range(0,1001,200)))\n",
    "ax.set_ylabel('elo')\n",
    "ax.set_yticks(list(range(0, 2001, 500)))\n",
    "\n",
    "plt.title(title2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed2598e-4e4f-4962-9543-d830353c039a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Task\n",
    "\n",
    "Redo the previous simulation with the rho difference between 2 players as a parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0f5aa8-29c3-4df6-b43e-d3122d7a6773",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "from utils import init_results , track_results\n",
    "\n",
    "# meta\n",
    "title22 = \"Elo Evolution with a rho differemce of {diff}\"\n",
    "results[title22] = {}\n",
    "\n",
    "# model specification\n",
    "mean_rho = 1000\n",
    "diffs = [0, 400, 800, 1000]\n",
    "\n",
    "# test ranking\n",
    "elo22 = ...\n",
    "\n",
    "# simulation params\n",
    "depth = ...\n",
    "solver = ...\n",
    "\n",
    "# simulation\n",
    "for diff in diffs:\n",
    "    # generate players\n",
    "    a, b = ...\n",
    "    \n",
    "    # add them to the elo system\n",
    "    ...\n",
    "    \n",
    "    # initialize results report\n",
    "    results[title22][diff] = init_results(pop=[a,b], ranking=elo22)\n",
    "\n",
    "    # run\n",
    "    for _ in range(depth):\n",
    "        # generate game\n",
    "        ...\n",
    "\n",
    "        # play game\n",
    "        ...\n",
    "        \n",
    "        # uodate elo\n",
    "        ...\n",
    "        \n",
    "        # track results\n",
    "        track_results(pop=[a,b], ranking=elo22, results=results[title22][diff])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80bdfb0-0efd-4665-b237-07ccebc17030",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PLOT\n",
    "fig, axes = plt.subplots(2,2, constrained_layout=True, figsize=(10, 5))\n",
    "for (diff, data), ax in zip(results[title22].items(), axes.reshape(-1)):\n",
    "    for player, color in zip(data.keys(), ['red', 'blue']):\n",
    "        ax.plot(x, data[player]['elo'], color=color)\n",
    "        ax.plot(x, data[player]['rho'], ':', color=color)\n",
    "\n",
    "    ax.set_xlabel('games')\n",
    "    ax.set_xticks([0, 200, 400, 600, 800, 1000])\n",
    "    ax.set_ylabel('elo')\n",
    "    ax.set_yticks([0, 500, 1000, 1500, 2000])\n",
    "    ax.set_title(title22.format(diff=diff), fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dfa4569b-9069-4bad-96ea-2066fce3888f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 3. N=3 players\n",
    "\n",
    "In rstt, there is a strict distinction between:\n",
    "- game generation or **Scheduler**: The package provides implementation of [Random Matchmaking](https://rstt.readthedocs.io/en/latest/rstt.scheduler.tournament.html#rstt.scheduler.tournament.random.RandomRound) and [RoundRobin](https://rstt.readthedocs.io/en/latest/rstt.scheduler.tournament.html#rstt.scheduler.tournament.groups.RoundRobin).\n",
    "- outcome generation or **Solver**: with the [ScoreProb](https://rstt.readthedocs.io/en/latest/rstt.solver.html#rstt.solver.solvers.ScoreProb) the user can specify a probability for each potential score of an encounter.\n",
    "- data preprocessing or **Observer**: a component used by a ranking to deal with the .update() method inputs. The package provides a [GamyByGame](https://rstt.readthedocs.io/en/latest/rstt.ranking.observer.html#rstt.ranking.observer.gameObserver.GameByGame) and a [BatchGame](https://rstt.readthedocs.io/en/latest/rstt.ranking.observer.html#rstt.ranking.observer.gameObserver.BatchGame) which are needed here.\n",
    "\n",
    "First we need the solver. We implement a parametrizable 'matrix solver'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b513067-8ac0-443b-888f-e00a910bd9eb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# READ\n",
    "from rstt.stypes import SPlayer\n",
    "from rstt.solver.solvers import ScoreProb, WIN, LOSE\n",
    "\n",
    "import random\n",
    "\n",
    "# Simulation Model - Game Outcome Generator\n",
    "class MatrixSolver(ScoreProb):\n",
    "    def __init__(self, matrix: dict[SPlayer, dict[SPlayer, float]]):\n",
    "        # matrix[p1][p2] := probabilitiy(p1 WIN against p2)\n",
    "        self.matrix = matrix\n",
    "        \n",
    "        # restrict games outcome to win or lose. We do not have draw or partial wins.\n",
    "        super().__init__(scores=[WIN, LOSE], func=self.expectedScore)\n",
    "\n",
    "    def expectedScore(self, duel: Duel) -> list[float]:\n",
    "        p1 = duel.player1()\n",
    "        p2 = duel.player2()\n",
    "        \n",
    "        # return the probability of each potential game score i.e [WIN, LOSE]\n",
    "        return [self.matrix[p1][p2], self.matrix[p2][p1]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f6af83b-1f36-4a91-b871-ca01f136813a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Observer: updated after each tournament versus each game\n",
    "(Eq. 35) is a en example of (Eq. 13) ratings computation for two games outcomes. The difference between 'updated after each tournament' and 'updated after each game' has to do with the function input and whether you apply:\n",
    "- (Eq. 13) iteratively on each single game score, thus using effectively (Eq. 1).\n",
    "- (Eq. 13) iteratively on each player with all its game results at once, hence (Eq. 35).\n",
    "\n",
    "The handler attribute of a ranking is responsible to format the ranking.update input (referred to 'observations') and call the ranking.backend.rate method to produce new ratings. The BasicElo ranking class uses the GameByGame Observer as default handler, i.e correspond to 'update after each game'. The BatchGame calls the rate method with a player and its corresponding results in the input.\n",
    "\n",
    "The case of the random matchmaking is not related with the handler of a ranking. It is an alternative to the RoundRobin scheduler. Since only one game is played per tournament, it does not matter if it uses the GameByGame or BatchGame observer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0576c4e-d54c-424c-a0ef-14f0bb9f8648",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FEATURE - modify the handler attribute\n",
    "\n",
    "from rstt.ranking import BatchGame\n",
    "\n",
    "# Baseline\n",
    "elo31 = BasicElo('randoms games system')\n",
    "\n",
    "# update after each tournament - BatchGame\n",
    "elo32 = BasicElo('RR tournaments (updated after each tournament ends)')\n",
    "elo32.handler = BatchGame()\n",
    "\n",
    "# update after each game - GameByGame\n",
    "elo33 = BasicElo('RR tournaments (updated after each single match)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2693e7c1-5bd4-48bf-beac-cab04863c914",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Game Data\n",
    "The package offers a wide collection of popular game generation method, including RoundRobin. The RandomRound generates random games. The user can specify the number of rounds to play, and the number of games in each rounds. In our case we want one game per tournament edition, i.e one round of one game, which are the default parameters.\n",
    "\n",
    "**Remark**:\n",
    "Competition, like RoundRobin and RandomRound, needs a seeding parameter (a Ranking) which is used to decide which games are generated. \n",
    "In both our cases, the seeding will not have an effect on which game are created. However in the RoundRobin, the ordering of the game can be modified by the seedings which may affect the 'update after each single match' scenario.\n",
    "\n",
    "The choice made in this tutorial is to set a strict experimental protocol that allows user to fine tune the exact simulation they want to perform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ed1d9f-f7f6-47bb-b783-a347ccd3903c",
   "metadata": {},
   "source": [
    "#### Task\n",
    "\n",
    "Read experiment() code. Make sure you understand the way data are produced and processed. Afterwards, feel free to make adjustement.\n",
    "\n",
    "- Do you understand why the ordering of mathups within a tournament could be different from one edition to the other one ?\n",
    "\n",
    "**YOUR ANSWER**: ...\n",
    "\n",
    "- Do you see how to make it a constant ?\n",
    "\n",
    "**YOUR ANSWER**: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49978ef1-ff06-4b9c-91d3-d52281a81c0e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# READ\n",
    "\n",
    "from rstt.stypes import Solver\n",
    "from rstt import Ranking, Competition\n",
    "\n",
    "# let us also check package performance\n",
    "import tqdm\n",
    "\n",
    "def experiment(title: str, # title to track results\n",
    "               rankings: list[Ranking], # a list of ranking to test\n",
    "               solver: Solver, # a way to decide game score\n",
    "               tournament: Competition, # a way to produce encounter\n",
    "               depth: int # a number of tournament edition\n",
    "              ) -> dict[str, dict[str, list[float]]]: # results\n",
    "\n",
    "    # competition needs a seeder\n",
    "    seeder = rankings[0]\n",
    "\n",
    "    # track ratings\n",
    "    results = {ranking.name: {player.name(): [ranking.rating(player)]\n",
    "                              for player in seeder}\n",
    "              for ranking in rankings\n",
    "              }\n",
    "    \n",
    "    # simulation\n",
    "    for i in tqdm.tqdm(range(depth)):\n",
    "        # data production\n",
    "        cup = tournament(name=f\"{title} - {i}\", seeding=seeder, solver=solver)\n",
    "        cup.registration(seeder.players())\n",
    "        cup.run()\n",
    "\n",
    "        # update ratings\n",
    "        for ranking in rankings:\n",
    "            ranking.update(games=cup.games())\n",
    "\n",
    "            # result tracking\n",
    "            for player in seeder:\n",
    "                results[ranking.name][player.name()].append(ranking.rating(player))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a779a996-ddbf-4295-a720-1bbc9de52d7e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EXAMPLE\n",
    "\n",
    "from rstt import RoundRobin, RandomRound\n",
    "\n",
    "# specification\n",
    "title3 = \"Elo Evolution  after {depth} {method}\"\n",
    "\n",
    "# simulation params\n",
    "A, B, C = [BasicPlayer(name=name)for name in ['A', 'B', 'C']]\n",
    "initial_ratings = {A: 1200, B: 1100, C: 1000 }\n",
    "matrix = {A: {B: 2/3, C: 1/3}, B: {A: 1/3, C: 2/3}, C:{A: 2/3, B: 1/3}}\n",
    "n3 = MatrixSolver(matrix)\n",
    "depth = 400\n",
    "\n",
    "# data tracking\n",
    "results[title3] = {}\n",
    "\n",
    "# initialisation\n",
    "for ranking in [elo31, elo32, elo33]:\n",
    "    for player, rating in initial_ratings.items():\n",
    "        ranking.set_rating(player, rating)\n",
    "\n",
    "# simulation\n",
    "results[title3] = experiment(title3, [elo31], n3, RandomRound, depth)\n",
    "results[title3].update(experiment(title3, [elo32, elo33], n3, RoundRobin, depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085aa084-54a7-46e9-9b00-0b7be842b86a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PLOT\n",
    "\n",
    "x = [i for i in range(depth+1)]\n",
    "fig, axes = plt.subplots(3, constrained_layout=True, figsize=(10,10))\n",
    "exp_res = results[title3].keys()\n",
    "for ranking, ax in zip(exp_res, axes):\n",
    "    ax.plot(x, results[title3][ranking]['A'], color='blue')\n",
    "    ax.plot(x, results[title3][ranking]['B'], color='green')\n",
    "    ax.plot(x, results[title3][ranking]['C'], color='red')\n",
    "\n",
    "    ax.set_title(title3.format(depth=depth, method=ranking), fontsize=10, fontweight='bold')\n",
    "    ax.set_xlabel('games')\n",
    "    ax.set_xticks([0, 100, 200, 300, 400])\n",
    "    ax.set_ylabel('elo')\n",
    "    ax.set_yticks([700, 800, 900, 1100, 1300])\n",
    "    ax.set_ylim(bottom=680, top=1420)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c958716a-6dea-4f2c-b854-040f8b89c67c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Distinction between data/handler/backend\n",
    "Consider the 'randoms games system' case.\n",
    "\n",
    "An alternative coding would be to play one RandomRound of 400 rounds, each consisting in a single game.\n",
    "- If you iterate over the games to update rankings: BatchGame and GameByGame are equivalent.\n",
    "- If you use a GameByGame Observer it makes no difference to update by iterating on single game or pass the entire dataset at once.\n",
    "- If you pass the entire dataset: GameByGame and BatchGame **are not** equivalent.\n",
    "\n",
    "Another parametrization of the simulation, to compare ranking with equal amount of data, is to specify 3 games per random rounds. In this scenario, using an elo rating system with a GameByGame or a BatchGame handler are different.\n",
    "\n",
    "**And the backend?**\n",
    "Well the math behind rating inference is distinct from the data procedure. You could test a GlickoRating system with a GameByGame or BatchGame Observer on random games or round-robins aswell. Do you want to add it in a comparative study?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1456c6df-0c34-4e30-9141-918072e64bb5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Task\n",
    "\n",
    "- Redo the 'Elo evolution with randoms games system' but with 3 games per round.\n",
    "- Compare an 'update after each tournaments' with 'update after each single game' using the same data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ff7e9d-f499-46e8-accf-607b422af37f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO \n",
    "\n",
    "# test ranking with an update by game approach\n",
    "rmgbg = ...\n",
    "# test ranking with an update by tournament approach\n",
    "rmbg = ...\n",
    "\n",
    "# initialisation\n",
    "for ranking in [rmgbg, rmbg]:\n",
    "    for player, rating in initial_ratings.items():\n",
    "        ranking.set_rating(player, rating)\n",
    "\n",
    "# RandomRound with 3 games per round\n",
    "MM3 = ... \n",
    "\n",
    "# run simulation\n",
    "results[title3].update(experiment(title3, ...))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84699d47-4bbb-4a73-b122-a02809dc77bb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PLOT\n",
    "x = [i for i in range(depth+1)]\n",
    "fig, axes = plt.subplots(len(results[title3].keys()), constrained_layout=True, figsize=(10,10))\n",
    "exp_res = results[title3].keys()\n",
    "for ranking, ax in zip(exp_res, axes):\n",
    "    ax.plot(x, results[title3][ranking]['A'], color='blue')\n",
    "    ax.plot(x, results[title3][ranking]['B'], color='green')\n",
    "    ax.plot(x, results[title3][ranking]['C'], color='red')\n",
    "\n",
    "    ax.set_title(title3.format(depth=depth, method=ranking), fontsize=10, fontweight='bold')\n",
    "    ax.set_xlabel('games')\n",
    "    ax.set_xticks([0, 100, 200, 300, 400])\n",
    "    ax.set_ylabel('elo')\n",
    "    ax.set_yticks([700, 800, 900, 1100, 1300])\n",
    "    ax.set_ylim(bottom=680, top=1420)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4525b60-4086-4dd0-8397-622a6ca79386",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 4. Round-robin tournament for N >> 1\n",
    "\n",
    "#### Task\n",
    "At this stage you have learned everything needed to replicate both diagram about the ELO mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db79bd6-8a26-43d7-a1a8-bf7fcb6a38f3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO / FEATURE - seeded_players\n",
    "\n",
    "from statistics import mean\n",
    "\n",
    "# experiment title\n",
    "title4 = \"Elo mean of 100 players in RRT {method}\"\n",
    "\n",
    "# create competitors\n",
    "levels = list(range(600, 1600, 10))\n",
    "population = BasicPlayer.seeded_players(nb=100, start=600, inc=10)\n",
    "\n",
    "# test rankings\n",
    "elo41 = ...\n",
    "elo42 = ...\n",
    "\n",
    "# simulation run\n",
    "depth = ...\n",
    "results[title4] = experiment(title4, ...)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d91b02c6-2433-4513-a669-9cf8698d750b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Ranking and Ground-Truth\n",
    "\n",
    "Ranking is not just a system to process data and infer rating. It is a container of player offering an hybrid list/dict interface where the elements are automaticaly sorted based on their ratings.\n",
    "Additionaly to standard ranking like Elo or Glicko, the package provides a [BTRanking](https://rstt.readthedocs.io/en/latest/rstt.ranking.standard.html#rstt.ranking.standard.consensus.BTRanking) which ranks player based on their levels. This means that for simulation using a LogSolver, it serves as the groundtruth.\n",
    "\n",
    "This is convenient here as we want to plot the player's rating based on their strenght in the simulation. Take a look at how we sort players_rho and elos_means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782c687c-b538-4988-acf7-ffc330f79b0c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PLOT / FEATURE - ranking slice are great for plot\n",
    "\n",
    "from rstt import BTRanking\n",
    "from statistics import mean\n",
    "\n",
    "\n",
    "gt4 = BTRanking('Consensus', population)\n",
    "for ranking in [elo41, elo42]:\n",
    "    fig, ax = plt.subplots(figsize=(10,4))\n",
    "\n",
    "    x =list(range(len(gt4)))\n",
    "\n",
    "    # Here is a nice 'slicing ranking' feature.\n",
    "    players_rho = [player.level() for player in gt4[::-1]] \n",
    "    elos_means = [mean(results[title4][ranking.name][player.name()]) for player in gt4[::-1]]\n",
    "\n",
    "    ax.plot(x, elos_means, 'bo', markerfacecolor='none', label='ELO mean calculated per player')\n",
    "    ax.plot(x, players_rho, color='red', label='player rho')\n",
    "\n",
    "    ax.set_xlabel('player')\n",
    "    ax.set_xticks([0,20,40,60, 80,100])\n",
    "    ax.set_ylabel('ELO mean')\n",
    "    ax.set_yticks([600, 800, 1000, 1200, 1400, 1600])\n",
    "\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title(title4.format(method=ranking.name))\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e29d74a2-a79a-46d5-9a98-bafb3138ea95",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Task\n",
    "\n",
    "Using the previous results and the Groundtruth ranking, select the correct player to reproduce the 'Elo Evolution and rho convergence for N players'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe930f25-7e72-4c91-bc74-a4f95f3e0aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# Which player to you want to see the elo convergence\n",
    "target = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7ee133-be73-48bd-a12a-e646de8eed53",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PLOT\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "x = list(range(depth+1))\n",
    "for p in target:\n",
    "    line, =plt.plot(x, results[title4][elo42.name][p.name()])\n",
    "    dashed = plt.plot(x, [p.level() for i in x], '--', color=line.get_color())\n",
    "\n",
    "ax.set_xlabel('tournament')\n",
    "ax.set_ylabel('elo')\n",
    "ax.set_title('Elo evolution and rho convergence for N players')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d39dd8-7a03-4027-bf36-43e31513e40b",
   "metadata": {},
   "source": [
    "### Consensus Ranking Implementation\n",
    "\n",
    "The notion of ranking in RSTT has some sort of formal definition, it consist of:\n",
    "- A **standing**: an ordered list of (rank, player, points)\n",
    "- A **datamodel**: a mapping Player -> Rating. It also providal an ordinal(Rating) -> float function\n",
    "- A **backend**: A statistical inference system so to speak, a function that produce Rating\n",
    "- A **handler**: We have already covered it, it is an input processor.\n",
    "\n",
    "Implicitly a Ranking is also defined by its **Rating** object, and the **Observations** that justify an update and can be processed by the handler.\n",
    "\n",
    "By Consensus Ranking we mean a Ranking that capture the simulation model strenght relationship between involved palyers. It is frequently convenient to have such one to compare infered results from a test ranking with **it should look like**.\n",
    "\n",
    "If you are interested in how to build Ranking have a look at Tutorial 3 that show how to use a external package as a rating system, or Tutorial 4 that look into modeling.\n",
    "\n",
    "In short, we build a Consensus ranking computing ratings with a Player object as input and output the sum of its win probabilities in a  MatrixSolver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eea0a2-c9ad-4a84-953e-83f22c5ae054",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# READ\n",
    "\n",
    "from rstt import Ranking\n",
    "from rstt.ranking import KeyModel, GaussianModel, PlayerChecker\n",
    "from rstt.ranking.rating import GlickoRating\n",
    "import statistics\n",
    "\n",
    "class MatrixLevel():\n",
    "    def __init__(self, solver: MatrixSolver):\n",
    "        self.solver = solver \n",
    "        \n",
    "    def rate(self, player: SPlayer) -> GlickoRating:\n",
    "        probabilities = self.solver.matrix[player].values()\n",
    "        return statistics.mean(probabilities)\n",
    "\n",
    "class MatrixRanking(Ranking):\n",
    "    def __init__(self, name: str, solver: MatrixSolver):\n",
    "        backend = MatrixLevel(solver)\n",
    "        super().__init__(name=name,\n",
    "                         datamodel=KeyModel(default=0),\n",
    "                         handler=PlayerChecker(),\n",
    "                         backend=backend,\n",
    "                         players=list(solver.matrix.keys()))\n",
    "\n",
    "        self.update()\n",
    "        \n",
    "    def forward(self, *args, **kwargs):\n",
    "        self.handler.handle_observations(players=self.players(), datamodel=self.datamodel, infer=self.backend)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f10bfe-a6af-40ac-ae6b-5d1501426880",
   "metadata": {},
   "source": [
    "#### Task\n",
    "\n",
    "Redo the *Means of ELO for P free sorted by theorical power*\n",
    "- Generate 200 players,\n",
    "- Use the MatrixSolver with a unfirom probability matrix\n",
    "- Use the MatrixRanking to sort player by power (for ploting)\n",
    "- Use an Elo Ranking with a BatchGame Observer\n",
    "- run the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1961340c-4d72-4b8c-b12b-449f068fdcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "free_title = \"Means of ELO for P free {consensus}\"\n",
    "\n",
    "# model\n",
    "free_pop = ...\n",
    "free_matrix = ...\n",
    "free_solver = MatrixSolver(free_matrix)\n",
    "\n",
    "# consensus ranking\n",
    "free_gt = MatrixRanking(name='Sorted by theoretical Power', solver=free_solver)\n",
    "\n",
    "# test ranking\n",
    "free_elo = ...\n",
    "\n",
    "# simulation\n",
    "depth = 500\n",
    "results[free_title] = experiment(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b6d77b-f72e-42ca-af4d-0abb67236ae4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PLOT\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "powers = list(range(0, len(free_gt)))\n",
    "elos_mean = [mean(results[free_title][free_elo.name][p.name()]) for p in free_gt[::-1]]\n",
    "ax.plot(powers, elos_mean, 'bo', markerfacecolor='none', label='ELO mean calculated per player')\n",
    "\n",
    "ax.set_xlabel('power')\n",
    "ax.set_xticks([0, 50, 100, 150, 200])\n",
    "ax.set_ylabel('mean elo')\n",
    "ax.set_yticks([1400, 1450, 1500, 1550, 1600, 1650])\n",
    "\n",
    "plt.title(free_title.format(consensus=free_gt.name))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee30a77a-974f-4541-8537-3ca84dff2c2f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 5. Research Extention\n",
    "\n",
    "Congratulation, you conducted your first simulation in RSTT. However, you have not seen experienced the greatest assets of the package.\n",
    "RSTT excells by its modularity that enables comparative studies and quick changes in model specification.\n",
    "You are now invited to make little changes to the code by modify some of the simulation parameters. \n",
    "\n",
    "- Change the population, try **Time Varing Strenght player**, use .update_level() to modify their strenght and .level() method to track *rho* values or a Consensus ranking.\n",
    "- Change BasicElo to **BasicGlicko** and track ranking.point(player) instead of .rating().\n",
    "- Try **SwissRound** instead of RoundRobin.\n",
    "- Explore other metric like rank correlation between consensus and test ranking, or the maximal rank difference of players over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53d64a9-431b-4dfc-bbd8-113020b353dd",
   "metadata": {},
   "source": [
    "#### Task\n",
    "- Change the population, try **Time Varing Strenght player**, use .update_level() to modify their strenght and .level() method to track *rho* values or a Consensus ranking.\n",
    "- Change BasicElo to **BasicGlicko** and track ranking.point(player) instead of .rating().\n",
    "- Try **SwissRound** instead of RoundRobin.\n",
    "- Explore other metric like rank correlation between consensus and test ranking, or the maximal rank difference of players over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925ed24c-ff72-414b-b2e1-6874c0fa7c65",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Have Fun!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
